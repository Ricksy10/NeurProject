{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chlorella Classification Pipeline - Cloud Notebook\n",
    "\n",
    "Ez a notebook egy teljes k√∂r≈± oszt√°lyoz√°si pipeline holografikus mikroszk√≥pos k√©pekhez.\n",
    "\n",
    "## Tartalom:\n",
    "1. Setup √©s k√∂rnyezet konfigur√°ci√≥\n",
    "2. Adatok bet√∂lt√©se √©s feldolgoz√°sa\n",
    "3. Model defin√≠ci√≥ (ResNet18/ResNeXt-50/VGG11-BN)\n",
    "4. Training with K-Fold Cross-Validation\n",
    "5. Evaluation √©s metrik√°k\n",
    "6. Inference √©s submission gener√°l√°s\n",
    "\n",
    "## Kaggle/Colab √∫tmutat√≥:\n",
    "- **Kaggle**: Az adatok a `/kaggle/input/your-dataset-name/` mapp√°ban v√°rhat√≥ak\n",
    "- **Colab**: T√∂ltsd fel az adatokat vagy csatold Google Drive-ot\n",
    "- A modell checkpointok a `/kaggle/working/outputs/` vagy `/content/outputs/` mapp√°ba ker√ºlnek\n",
    "\n",
    "## ‚ö†Ô∏è FONTOS - √Åll√≠tsd be az adatok el√©r√©si √∫tj√°t:\n",
    "A notebook 3. cell√°j√°ban (Konfigur√°ci√≥) m√≥dos√≠tsd a `data_root` √©rt√©k√©t a saj√°t adataid hely√©re!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup √©s Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import alapvet≈ë library-k\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import ssl\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Any, Optional, Callable\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torchvision import models\n",
    "from PIL import Image\n",
    "import albumentations as A\n",
    "\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from sklearn.metrics import (\n",
    "    fbeta_score, \n",
    "    precision_recall_fscore_support, \n",
    "    confusion_matrix,\n",
    "    precision_recall_curve\n",
    ")\n",
    "\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úì Imports sikeres!\")\n",
    "print(f\"PyTorch verzi√≥: {torch.__version__}\")\n",
    "print(f\"CUDA el√©rhet≈ë: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Utility Funkci√≥k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============== KONFIGUR√ÅCI√ì ===============\n",
    "# ‚ö†Ô∏è FONTOS: M√≥dos√≠tsd a 'data_root' √©rt√©k√©t a saj√°t adataid hely√©re!\n",
    "\n",
    "CONFIG = {\n",
    "    'data': {\n",
    "        'data_root': '/kaggle/input/itk-nn',  # üëà M√ìDOS√çTSD ezt!\n",
    "        # Kaggle p√©lda: '/kaggle/input/your-dataset-name'\n",
    "        # Colab p√©lda: '/content/your-dataset-folder'\n",
    "        'output_dir': '/kaggle/working/outputs',  # Colab: '/content/outputs'\n",
    "        'img_size': 224,\n",
    "        'num_workers': 2  # Cloud k√∂rnyezetben kevesebb worker\n",
    "    },\n",
    "    'model': {\n",
    "        'architecture': 'resnet18',  # 'resnet18', 'resnext50_32x4d', 'vgg11_bn'\n",
    "        'num_classes': 5,\n",
    "        'input_channels': 4,\n",
    "        'pretrained': True  # ‚ö†Ô∏è Kaggle-en kapcsold be az Internet-et a Settings-ben!\n",
    "    },\n",
    "    'training': {\n",
    "        'num_folds': 5,\n",
    "        'epochs': 20,  # Cloud k√∂rnyezetben kevesebb epoch\n",
    "        'batch_size': 16,\n",
    "        'lr_head': 0.001,\n",
    "        'lr_backbone': 0.0001,\n",
    "        'weight_decay': 0.0001,\n",
    "        'patience': 5,\n",
    "        'unfreeze_epoch': 5\n",
    "    },\n",
    "    'augmentation': {\n",
    "        'rotation_degrees': 10,\n",
    "        'horizontal_flip_prob': 0.5,\n",
    "        'vertical_flip_prob': 0.5,\n",
    "        'brightness': 0.2,\n",
    "        'contrast': 0.2,\n",
    "        'blur_prob': 0.3,\n",
    "        'blur_sigma_min': 0.1,\n",
    "        'blur_sigma_max': 2.0\n",
    "    },\n",
    "    'reproducibility': {\n",
    "        'seed': 42\n",
    "    }\n",
    "}\n",
    "\n",
    "# Oszt√°ly defin√≠ci√≥k\n",
    "CLASS_LABELS = [\n",
    "    {'label_id': 0, 'label_name': 'chlorella', 'folder_name': 'class_chlorella', 'is_priority': True},\n",
    "    {'label_id': 1, 'label_name': 'debris', 'folder_name': 'class_debris', 'is_priority': False},\n",
    "    {'label_id': 2, 'label_name': 'haematococcus', 'folder_name': 'class_haematococcus', 'is_priority': False},\n",
    "    {'label_id': 3, 'label_name': 'small_haematococcus', 'folder_name': 'class_small_haemato', 'is_priority': False},\n",
    "    {'label_id': 4, 'label_name': 'small_particle', 'folder_name': 'class_small_particle', 'is_priority': False},\n",
    "]\n",
    "\n",
    "CLASS_ID_TO_NAME = {cls['label_id']: cls['label_name'] for cls in CLASS_LABELS}\n",
    "FOLDER_TO_CLASS_ID = {cls['folder_name']: cls['label_id'] for cls in CLASS_LABELS}\n",
    "\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "# SSL fix macOS-hez (ha sz√ºks√©ges)\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "print(\"‚úì Konfigur√°ci√≥ bet√∂ltve!\")\n",
    "print(f\"üìÇ Adatok helye: {CONFIG['data']['data_root']}\")\n",
    "print(f\"üèóÔ∏è  Architekt√∫ra: {CONFIG['model']['architecture']}\")\n",
    "print(f\"üì¶ Batch size: {CONFIG['training']['batch_size']}\")\n",
    "print(f\"üîÑ Epochs: {CONFIG['training']['epochs']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Konfigur√°ci√≥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============== UTILITY FUNKCI√ìK ===============\n",
    "\n",
    "def set_seed(seed: int = 42):\n",
    "    \"\"\"Random seed be√°ll√≠t√°sa reproduk√°lhat√≥s√°ghoz\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def ensure_dir(directory: str) -> Path:\n",
    "    \"\"\"K√∂nyvt√°r l√©trehoz√°sa, ha nem l√©tezik\"\"\"\n",
    "    dir_path = Path(directory)\n",
    "    dir_path.mkdir(parents=True, exist_ok=True)\n",
    "    return dir_path\n",
    "\n",
    "def parse_subject_id(filename: str) -> str:\n",
    "    \"\"\"Subject ID kinyer√©se f√°jln√©vb≈ël\"\"\"\n",
    "    basename = Path(filename).stem\n",
    "    pattern = r'^(.+?)_(?:amp|phase|mask)$'\n",
    "    match = re.match(pattern, basename)\n",
    "    return match.group(1) if match else basename\n",
    "\n",
    "def discover_subjects(data_root: str, split: str = 'train') -> Dict[str, Dict]:\n",
    "    \"\"\"K√©pek felfedez√©se √©s csoportos√≠t√°sa subject ID √©s modalit√°s szerint\"\"\"\n",
    "    data_root = Path(data_root)\n",
    "    split_dir = data_root / split\n",
    "    \n",
    "    if not split_dir.exists():\n",
    "        raise FileNotFoundError(f\"Split k√∂nyvt√°r nem tal√°lhat√≥: {split_dir}\")\n",
    "    \n",
    "    subjects = {}\n",
    "    \n",
    "    if split == 'train':\n",
    "        for class_folder in sorted(split_dir.iterdir()):\n",
    "            if not class_folder.is_dir():\n",
    "                continue\n",
    "            \n",
    "            folder_name = class_folder.name\n",
    "            if folder_name not in FOLDER_TO_CLASS_ID:\n",
    "                continue\n",
    "            \n",
    "            class_id = FOLDER_TO_CLASS_ID[folder_name]\n",
    "            class_name = CLASS_ID_TO_NAME[class_id]\n",
    "            \n",
    "            for img_path in sorted(class_folder.glob('*.png')):\n",
    "                subject_id = parse_subject_id(img_path.name)\n",
    "                \n",
    "                modality = None\n",
    "                if '_amp' in img_path.stem:\n",
    "                    modality = 'amp'\n",
    "                elif '_phase' in img_path.stem:\n",
    "                    modality = 'phase'\n",
    "                elif '_mask' in img_path.stem:\n",
    "                    modality = 'mask'\n",
    "                else:\n",
    "                    continue\n",
    "                \n",
    "                if subject_id not in subjects:\n",
    "                    subjects[subject_id] = {\n",
    "                        'subject_id': subject_id,\n",
    "                        'class_label': class_id,\n",
    "                        'class_name': class_name,\n",
    "                        'modalities': {},\n",
    "                        'split': split\n",
    "                    }\n",
    "                \n",
    "                subjects[subject_id]['modalities'][modality] = img_path\n",
    "    \n",
    "    else:  # test\n",
    "        # Test eset√©n egy f√°jl = egy subject (nincs modalit√°s szepar√°ci√≥)\n",
    "        png_files = list(split_dir.glob('*.png'))\n",
    "        \n",
    "        if not png_files:\n",
    "            png_files = list(split_dir.glob('**/*.png'))\n",
    "        \n",
    "        print(f\"‚úì {len(png_files)} test k√©p tal√°lhat√≥\")\n",
    "        \n",
    "        for img_path in sorted(png_files):\n",
    "            # Subject ID = f√°jln√©v .png n√©lk√ºl\n",
    "            subject_id = img_path.stem\n",
    "            \n",
    "            # Test eset√©n egy k√©p tartalmazza az √∂sszes modalit√°st\n",
    "            # Mindh√°rom modalit√°snak ugyanazt a k√©pet haszn√°ljuk\n",
    "            subjects[subject_id] = {\n",
    "                'subject_id': subject_id,\n",
    "                'class_label': None,\n",
    "                'class_name': None,\n",
    "                'modalities': {\n",
    "                    'amp': img_path,\n",
    "                    'phase': img_path,\n",
    "                    'mask': img_path\n",
    "                },\n",
    "                'split': split\n",
    "            }\n",
    "    \n",
    "    return subjects\n",
    "\n",
    "def create_subject_folds(subject_ids: List[str], class_labels: List[int], \n",
    "                         n_splits: int = 5, seed: int = 42):\n",
    "    \"\"\"K-Fold split k√©sz√≠t√©se StratifiedGroupKFold-dal\"\"\"\n",
    "    sgkf = StratifiedGroupKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "    \n",
    "    X = np.arange(len(subject_ids))\n",
    "    y = np.array(class_labels)\n",
    "    groups = np.array(subject_ids)\n",
    "    \n",
    "    folds = []\n",
    "    for train_idx, val_idx in sgkf.split(X, y, groups):\n",
    "        train_subjects = [subject_ids[i] for i in train_idx]\n",
    "        val_subjects = [subject_ids[i] for i in val_idx]\n",
    "        folds.append((train_subjects, val_subjects))\n",
    "    \n",
    "    return folds\n",
    "\n",
    "# Seed be√°ll√≠t√°sa\n",
    "set_seed(CONFIG['reproducibility']['seed'])\n",
    "print(\"‚úì Utility funkci√≥k bet√∂ltve √©s seed be√°ll√≠tva!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============== MODEL √âP√çT√âS ===============\n",
    "\n",
    "def build_backbone(architecture: str = 'resnet18', pretrained: bool = True):\n",
    "    \"\"\"Pre-trained backbone bet√∂lt√©se\"\"\"\n",
    "    if architecture == 'resnet18':\n",
    "        model = models.resnet18(pretrained=pretrained)\n",
    "        feature_dim = model.fc.in_features\n",
    "        model = nn.Sequential(*list(model.children())[:-1])\n",
    "        return model, feature_dim\n",
    "    \n",
    "    elif architecture == 'resnext50_32x4d':\n",
    "        model = models.resnext50_32x4d(pretrained=pretrained)\n",
    "        feature_dim = model.fc.in_features\n",
    "        model = nn.Sequential(*list(model.children())[:-1])\n",
    "        return model, feature_dim\n",
    "    \n",
    "    elif architecture == 'vgg11_bn':\n",
    "        model = models.vgg11_bn(pretrained=pretrained)\n",
    "        feature_extractor = model.features\n",
    "        feature_dim = 512\n",
    "        return feature_extractor, feature_dim\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Nem t√°mogatott architekt√∫ra: {architecture}\")\n",
    "\n",
    "def adapt_first_conv_for_4ch(model: nn.Module, architecture: str = 'resnet18'):\n",
    "    \"\"\"Els≈ë conv r√©teg adapt√°l√°sa 4 csatorn√°s inputra\"\"\"\n",
    "    if architecture in ['resnet18', 'resnext50_32x4d']:\n",
    "        old_conv = model[0]\n",
    "        \n",
    "        new_conv = nn.Conv2d(\n",
    "            in_channels=4,\n",
    "            out_channels=old_conv.out_channels,\n",
    "            kernel_size=old_conv.kernel_size,\n",
    "            stride=old_conv.stride,\n",
    "            padding=old_conv.padding,\n",
    "            bias=old_conv.bias is not None\n",
    "        )\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            new_conv.weight[:, :3, :, :] = old_conv.weight\n",
    "            new_conv.weight[:, 3:, :, :] = torch.randn_like(new_conv.weight[:, 3:, :, :]) * 0.01\n",
    "            if old_conv.bias is not None:\n",
    "                new_conv.bias.copy_(old_conv.bias)\n",
    "        \n",
    "        model[0] = new_conv\n",
    "    \n",
    "    elif architecture == 'vgg11_bn':\n",
    "        old_conv = model[0]\n",
    "        \n",
    "        new_conv = nn.Conv2d(\n",
    "            in_channels=4,\n",
    "            out_channels=old_conv.out_channels,\n",
    "            kernel_size=old_conv.kernel_size,\n",
    "            stride=old_conv.stride,\n",
    "            padding=old_conv.padding,\n",
    "            bias=old_conv.bias is not None\n",
    "        )\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            new_conv.weight[:, :3, :, :] = old_conv.weight\n",
    "            new_conv.weight[:, 3:, :, :] = torch.randn_like(new_conv.weight[:, 3:, :, :]) * 0.01\n",
    "            if old_conv.bias is not None:\n",
    "                new_conv.bias.copy_(old_conv.bias)\n",
    "        \n",
    "        model[0] = new_conv\n",
    "    \n",
    "    return model\n",
    "\n",
    "def replace_classifier_head(model: nn.Module, architecture: str, feature_dim: int, num_classes: int = 5):\n",
    "    \"\"\"Classifier fej cser√©je\"\"\"\n",
    "    if architecture in ['resnet18', 'resnext50_32x4d']:\n",
    "        classifier = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(feature_dim, num_classes)\n",
    "        )\n",
    "        model = nn.Sequential(model, classifier)\n",
    "    \n",
    "    elif architecture == 'vgg11_bn':\n",
    "        classifier = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((7, 7)),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(feature_dim * 7 * 7, 4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096, num_classes)\n",
    "        )\n",
    "        model = nn.Sequential(model, classifier)\n",
    "    \n",
    "    return model\n",
    "\n",
    "class ChlorellaClassifier(nn.Module):\n",
    "    \"\"\"Teljes classifier modell\"\"\"\n",
    "    \n",
    "    def __init__(self, architecture: str = 'resnet18', num_classes: int = 5, \n",
    "                 input_channels: int = 4, pretrained: bool = True):\n",
    "        super().__init__()\n",
    "        self.architecture = architecture\n",
    "        self.num_classes = num_classes\n",
    "        self.input_channels = input_channels\n",
    "        \n",
    "        # Model √©p√≠t√©se\n",
    "        backbone, feature_dim = build_backbone(architecture, pretrained)\n",
    "        backbone = adapt_first_conv_for_4ch(backbone, architecture)\n",
    "        self.model = replace_classifier_head(backbone, architecture, feature_dim, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def get_backbone_params(self):\n",
    "        \"\"\"Backbone param√©terek (discriminative fine-tuning-hoz)\"\"\"\n",
    "        return self.model[0].parameters()\n",
    "    \n",
    "    def get_classifier_params(self):\n",
    "        \"\"\"Classifier param√©terek\"\"\"\n",
    "        return self.model[1].parameters()\n",
    "\n",
    "print(\"‚úì Model architekt√∫ra defini√°lva!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============== TRAINING LOOP ===============\n",
    "\n",
    "def train_one_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    \"\"\"Egy epoch training\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for inputs, labels in tqdm(dataloader, desc='Training'):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Skip invalid labels\n",
    "        valid_mask = labels >= 0\n",
    "        if not valid_mask.any():\n",
    "            continue\n",
    "        \n",
    "        inputs = inputs[valid_mask]\n",
    "        labels = labels[valid_mask]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item() * inputs.size(0)\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    avg_loss = total_loss / total if total > 0 else 0.0\n",
    "    accuracy = correct / total if total > 0 else 0.0\n",
    "    \n",
    "    return avg_loss, accuracy\n",
    "\n",
    "def validate(model, dataloader, criterion, device):\n",
    "    \"\"\"Valid√°ci√≥\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    all_probs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(dataloader, desc='Validation'):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            valid_mask = labels >= 0\n",
    "            if not valid_mask.any():\n",
    "                continue\n",
    "            \n",
    "            inputs = inputs[valid_mask]\n",
    "            labels = labels[valid_mask]\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "            _, predicted = outputs.max(1)\n",
    "            \n",
    "            total_loss += loss.item() * inputs.size(0)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "    \n",
    "    y_true = np.array(all_labels)\n",
    "    y_pred = np.array(all_preds)\n",
    "    y_probs = np.array(all_probs)\n",
    "    \n",
    "    avg_loss = total_loss / len(y_true) if len(y_true) > 0 else 0.0\n",
    "    accuracy = (y_true == y_pred).mean() if len(y_true) > 0 else 0.0\n",
    "    chlorella_f0_5 = compute_fbeta_score(y_true, y_pred, beta=0.5, class_id=0)\n",
    "    \n",
    "    return avg_loss, accuracy, chlorella_f0_5, y_true, y_pred, y_probs\n",
    "\n",
    "print(\"‚úì Training loop funkci√≥k defini√°lva!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============== TRAINING UTILITIES ===============\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"Early stopping az F0.5 metrika alapj√°n\"\"\"\n",
    "    \n",
    "    def __init__(self, patience: int = 5, mode: str = 'max', delta: float = 0.0):\n",
    "        self.patience = patience\n",
    "        self.mode = mode\n",
    "        self.delta = delta\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.should_stop = False\n",
    "    \n",
    "    def __call__(self, score: float) -> bool:\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            return False\n",
    "        \n",
    "        if self.mode == 'max':\n",
    "            improved = score > self.best_score + self.delta\n",
    "        else:\n",
    "            improved = score < self.best_score - self.delta\n",
    "        \n",
    "        if improved:\n",
    "            self.best_score = score\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.should_stop = True\n",
    "        \n",
    "        return self.should_stop\n",
    "\n",
    "def freeze_backbone(model: nn.Module):\n",
    "    \"\"\"Backbone befagyaszt√°sa\"\"\"\n",
    "    if hasattr(model, 'get_backbone_params'):\n",
    "        for param in model.get_backbone_params():\n",
    "            param.requires_grad = False\n",
    "\n",
    "def unfreeze_backbone(model: nn.Module):\n",
    "    \"\"\"Backbone felolvaszt√°sa\"\"\"\n",
    "    if hasattr(model, 'get_backbone_params'):\n",
    "        for param in model.get_backbone_params():\n",
    "            param.requires_grad = True\n",
    "\n",
    "def get_discriminative_optimizer(model: nn.Module, lr_head: float = 1e-3, \n",
    "                                 lr_backbone: float = 1e-4, weight_decay: float = 1e-4):\n",
    "    \"\"\"Optimizer k√ºl√∂nb√∂z≈ë learning rate-ekkel\"\"\"\n",
    "    if hasattr(model, 'get_backbone_params') and hasattr(model, 'get_classifier_params'):\n",
    "        optimizer = optim.Adam([\n",
    "            {'params': model.get_backbone_params(), 'lr': lr_backbone},\n",
    "            {'params': model.get_classifier_params(), 'lr': lr_head}\n",
    "        ], weight_decay=weight_decay)\n",
    "    else:\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr_head, weight_decay=weight_decay)\n",
    "    return optimizer\n",
    "\n",
    "def compute_fbeta_score(y_true: np.ndarray, y_pred: np.ndarray, beta: float = 0.5, class_id: int = 0):\n",
    "    \"\"\"F-beta score sz√°m√≠t√°sa egy oszt√°lyra\"\"\"\n",
    "    y_true_binary = (y_true == class_id).astype(int)\n",
    "    y_pred_binary = (y_pred == class_id).astype(int)\n",
    "    score = fbeta_score(y_true_binary, y_pred_binary, beta=beta, zero_division=0.0)\n",
    "    return float(score)\n",
    "\n",
    "print(\"‚úì Training utility funkci√≥k defini√°lva!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============== DATASET OSZT√ÅLY ===============\n",
    "\n",
    "class SubjectDataset(Dataset):\n",
    "    \"\"\"Multi-modal holografikus mikroszk√≥pos k√©pek dataset-je\"\"\"\n",
    "    \n",
    "    def __init__(self, subjects: Dict[str, Dict], transform=None, img_size: int = 224):\n",
    "        self.subjects = list(subjects.values())\n",
    "        self.transform = transform if transform else get_val_transforms(img_size)\n",
    "        self.img_size = img_size\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.subjects)\n",
    "    \n",
    "    def __getitem__(self, idx: int):\n",
    "        subject = self.subjects[idx]\n",
    "        modalities = subject['modalities']\n",
    "        \n",
    "        # Modalit√°sok bet√∂lt√©se vagy zero-fill\n",
    "        amp_img, amp_present = self._load_modality(modalities, 'amp')\n",
    "        phase_img, phase_present = self._load_modality(modalities, 'phase')\n",
    "        mask_img, mask_present = self._load_modality(modalities, 'mask')\n",
    "        \n",
    "        # Augment√°ci√≥k alkalmaz√°sa\n",
    "        if self.transform:\n",
    "            transformed = self.transform(image=amp_img, phase=phase_img, mask=mask_img)\n",
    "            amp_img = transformed['image']\n",
    "            phase_img = transformed['phase']\n",
    "            mask_img = transformed['mask']\n",
    "        \n",
    "        # NumPy array-ekk√© konvert√°l√°s\n",
    "        if isinstance(amp_img, Image.Image):\n",
    "            amp_img = np.array(amp_img)\n",
    "        if isinstance(phase_img, Image.Image):\n",
    "            phase_img = np.array(phase_img)\n",
    "        if isinstance(mask_img, Image.Image):\n",
    "            mask_img = np.array(mask_img)\n",
    "        \n",
    "        # Sz√ºrke√°rnyalatosra biztos√≠t√°s\n",
    "        amp_img = self._ensure_grayscale(amp_img)\n",
    "        phase_img = self._ensure_grayscale(phase_img)\n",
    "        mask_img = self._ensure_grayscale(mask_img)\n",
    "        \n",
    "        # Normaliz√°l√°s [0, 1]-re\n",
    "        amp_img = amp_img.astype(np.float32) / 255.0\n",
    "        phase_img = phase_img.astype(np.float32) / 255.0\n",
    "        mask_img = mask_img.astype(np.float32) / 255.0\n",
    "        \n",
    "        # 3 csatorn√°ba stackel√©s\n",
    "        img_3ch = np.stack([amp_img, phase_img, mask_img], axis=0)\n",
    "        \n",
    "        # ImageNet normaliz√°l√°s az els≈ë 3 csatorn√°ra\n",
    "        for i in range(3):\n",
    "            img_3ch[i] = (img_3ch[i] - IMAGENET_MEAN[i]) / IMAGENET_STD[i]\n",
    "        \n",
    "        # Mask indicator csatorna (4. csatorna)\n",
    "        mask_indicator = np.array([\n",
    "            float(amp_present), \n",
    "            float(phase_present), \n",
    "            float(mask_present)\n",
    "        ], dtype=np.float32).mean()\n",
    "        \n",
    "        mask_indicator_ch = np.full(\n",
    "            (1, img_3ch.shape[1], img_3ch.shape[2]), \n",
    "            mask_indicator, \n",
    "            dtype=np.float32\n",
    "        )\n",
    "        \n",
    "        # 4 csatorn√°s tensor √∂ssze√°ll√≠t√°sa\n",
    "        img_4ch = np.concatenate([img_3ch, mask_indicator_ch], axis=0)\n",
    "        tensor = torch.from_numpy(img_4ch).float()\n",
    "        \n",
    "        # Label\n",
    "        label = subject['class_label'] if subject['class_label'] is not None else -1\n",
    "        \n",
    "        return tensor, label\n",
    "    \n",
    "    def _load_modality(self, modalities: Dict[str, Path], modality_type: str):\n",
    "        \"\"\"Modalit√°s bet√∂lt√©se vagy zeros visszaad√°sa\"\"\"\n",
    "        if modality_type in modalities:\n",
    "            img_path = modalities[modality_type]\n",
    "            try:\n",
    "                img = Image.open(img_path).convert('L')\n",
    "                return np.array(img), True\n",
    "            except Exception as e:\n",
    "                print(f\"Figyelmeztet√©s: {img_path} bet√∂lt√©se sikertelen: {e}\")\n",
    "                return np.zeros((self.img_size, self.img_size), dtype=np.uint8), False\n",
    "        else:\n",
    "            return np.zeros((self.img_size, self.img_size), dtype=np.uint8), False\n",
    "    \n",
    "    def _ensure_grayscale(self, img: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Sz√ºrke√°rnyalatosra biztos√≠t√°s\"\"\"\n",
    "        if img.ndim == 3:\n",
    "            img = img.mean(axis=2)\n",
    "        elif img.ndim != 2:\n",
    "            raise ValueError(f\"V√°ratlan k√©p m√©ret: {img.shape}\")\n",
    "        return img\n",
    "\n",
    "print(\"‚úì Dataset oszt√°ly defini√°lva!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============== AUGMENT√ÅCI√ìK ===============\n",
    "\n",
    "def get_train_transforms(img_size: int = 224):\n",
    "    \"\"\"Training augment√°ci√≥s pipeline\"\"\"\n",
    "    cfg = CONFIG['augmentation']\n",
    "    return A.Compose([\n",
    "        A.Resize(height=img_size, width=img_size),\n",
    "        A.Rotate(limit=cfg['rotation_degrees'], p=0.5, border_mode=0),\n",
    "        A.HorizontalFlip(p=cfg['horizontal_flip_prob']),\n",
    "        A.VerticalFlip(p=cfg['vertical_flip_prob']),\n",
    "        A.ColorJitter(brightness=cfg['brightness'], contrast=cfg['contrast'], p=0.3),\n",
    "        A.GaussianBlur(blur_limit=(3, 7), \n",
    "                       sigma_limit=(cfg['blur_sigma_min'], cfg['blur_sigma_max']), \n",
    "                       p=cfg['blur_prob']),\n",
    "    ], additional_targets={'phase': 'image', 'mask': 'mask'})\n",
    "\n",
    "def get_val_transforms(img_size: int = 224):\n",
    "    \"\"\"Validation/test transform (csak resize)\"\"\"\n",
    "    return A.Compose([\n",
    "        A.Resize(height=img_size, width=img_size),\n",
    "    ], additional_targets={'phase': 'image', 'mask': 'mask'})\n",
    "\n",
    "print(\"‚úì Augment√°ci√≥k defini√°lva!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dataset √©s Augment√°ci√≥k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============== FOLD TRAINING ===============\n",
    "\n",
    "def train_one_fold(model, train_loader, val_loader, fold_id, config, device, output_dir):\n",
    "    \"\"\"Egy fold teljes training-je k√©t szakaszban\"\"\"\n",
    "    \n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    epochs = config['training']['epochs']\n",
    "    unfreeze_epoch = config['training']['unfreeze_epoch']\n",
    "    patience = config['training']['patience']\n",
    "    lr_head = config['training']['lr_head']\n",
    "    lr_backbone = config['training']['lr_backbone']\n",
    "    weight_decay = config['training']['weight_decay']\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    early_stopping = EarlyStopping(patience=patience, mode='max')\n",
    "    \n",
    "    best_f0_5 = 0.0\n",
    "    best_epoch = 0\n",
    "    best_val_predictions = None\n",
    "    \n",
    "    # Stage 1: Head-only training\n",
    "    print(f\"\\\\n[Fold {fold_id}] Stage 1: Classifier head training (backbone frozen)\")\n",
    "    freeze_backbone(model)\n",
    "    optimizer = optim.Adam(\n",
    "        filter(lambda p: p.requires_grad, model.parameters()), \n",
    "        lr=lr_head, \n",
    "        weight_decay=weight_decay\n",
    "    )\n",
    "    \n",
    "    for epoch in range(min(unfreeze_epoch, epochs)):\n",
    "        train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        val_loss, val_acc, chlorella_f0_5, y_true, y_pred, y_probs = validate(\n",
    "            model, val_loader, criterion, device\n",
    "        )\n",
    "        \n",
    "        print(f\"[Fold {fold_id}] Epoch {epoch+1}/{epochs} | \"\n",
    "              f\"Train Loss: {train_loss:.4f}, Acc: {train_acc:.4f} | \"\n",
    "              f\"Val Loss: {val_loss:.4f}, Acc: {val_acc:.4f}, F0.5: {chlorella_f0_5:.4f}\")\n",
    "        \n",
    "        if chlorella_f0_5 > best_f0_5:\n",
    "            best_f0_5 = chlorella_f0_5\n",
    "            best_epoch = epoch + 1\n",
    "            \n",
    "            checkpoint_path = output_dir / f'fold_{fold_id}_best.pth'\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'metric_value': chlorella_f0_5,\n",
    "                'config': config\n",
    "            }, checkpoint_path)\n",
    "            \n",
    "            best_val_predictions = {'y_true': y_true, 'y_pred': y_pred, 'y_probs': y_probs}\n",
    "            print(f\"[Fold {fold_id}] √öj legjobb F0.5: {chlorella_f0_5:.4f} ‚Üí Checkpoint mentve\")\n",
    "        \n",
    "        if early_stopping(chlorella_f0_5):\n",
    "            print(f\"[Fold {fold_id}] Early stopping triggered at epoch {epoch+1}\")\n",
    "            break\n",
    "    \n",
    "    # Stage 2: Full network fine-tuning\n",
    "    if epoch + 1 >= unfreeze_epoch and not early_stopping.should_stop:\n",
    "        print(f\"\\\\n[Fold {fold_id}] Stage 2: Full network fine-tuning (backbone unfrozen)\")\n",
    "        unfreeze_backbone(model)\n",
    "        optimizer = get_discriminative_optimizer(\n",
    "            model, lr_head=lr_head, lr_backbone=lr_backbone, weight_decay=weight_decay\n",
    "        )\n",
    "        early_stopping = EarlyStopping(patience=patience, mode='max')\n",
    "        \n",
    "        for epoch in range(unfreeze_epoch, epochs):\n",
    "            train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
    "            val_loss, val_acc, chlorella_f0_5, y_true, y_pred, y_probs = validate(\n",
    "                model, val_loader, criterion, device\n",
    "            )\n",
    "            \n",
    "            print(f\"[Fold {fold_id}] Epoch {epoch+1}/{epochs} | \"\n",
    "                  f\"Train Loss: {train_loss:.4f}, Acc: {train_acc:.4f} | \"\n",
    "                  f\"Val Loss: {val_loss:.4f}, Acc: {val_acc:.4f}, F0.5: {chlorella_f0_5:.4f}\")\n",
    "            \n",
    "            if chlorella_f0_5 > best_f0_5:\n",
    "                best_f0_5 = chlorella_f0_5\n",
    "                best_epoch = epoch + 1\n",
    "                \n",
    "                checkpoint_path = output_dir / f'fold_{fold_id}_best.pth'\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'metric_value': chlorella_f0_5,\n",
    "                    'config': config\n",
    "                }, checkpoint_path)\n",
    "                \n",
    "                best_val_predictions = {'y_true': y_true, 'y_pred': y_pred, 'y_probs': y_probs}\n",
    "                print(f\"[Fold {fold_id}] √öj legjobb F0.5: {chlorella_f0_5:.4f} ‚Üí Checkpoint mentve\")\n",
    "            \n",
    "            if early_stopping(chlorella_f0_5):\n",
    "                print(f\"[Fold {fold_id}] Early stopping triggered at epoch {epoch+1}\")\n",
    "                break\n",
    "    \n",
    "    print(f\"\\\\n[Fold {fold_id}] Training befejezve. Legjobb F0.5: {best_f0_5:.4f} at epoch {best_epoch}\")\n",
    "    \n",
    "    return {\n",
    "        'best_f0_5': best_f0_5,\n",
    "        'best_epoch': best_epoch,\n",
    "        'val_predictions': best_val_predictions\n",
    "    }\n",
    "\n",
    "print(\"‚úì Fold training funkci√≥ defini√°lva!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training Loop (√ñsszes Fold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Adatok Bet√∂lt√©se √©s K-Fold Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============== K-FOLD SPLITS L√âTREHOZ√ÅSA ===============\n",
    "\n",
    "# Subject-ek √©s labelek kigy≈±jt√©se\n",
    "subject_ids = list(all_subjects.keys())\n",
    "class_labels = [all_subjects[sid]['class_label'] for sid in subject_ids]\n",
    "\n",
    "# K-Fold splits\n",
    "num_folds = CONFIG['training']['num_folds']\n",
    "print(f\"\\nüîÄ {num_folds}-fold cross-validation splits l√©trehoz√°sa...\")\n",
    "\n",
    "folds = create_subject_folds(\n",
    "    subject_ids, \n",
    "    class_labels, \n",
    "    n_splits=num_folds, \n",
    "    seed=CONFIG['reproducibility']['seed']\n",
    ")\n",
    "\n",
    "print(f\"‚úì {len(folds)} fold elk√©sz√ºlt!\")\n",
    "\n",
    "# Fold-ok statisztik√°ja\n",
    "print(\"\\nüìà Fold-ok m√©rete:\")\n",
    "for fold_id, (train_sids, val_sids) in enumerate(folds):\n",
    "    train_count = len(train_sids)\n",
    "    val_count = len(val_sids)\n",
    "    split_ratio = (val_count / (train_count + val_count)) * 100\n",
    "    print(f\"  Fold {fold_id}: {train_count} train, {val_count} val ({split_ratio:.1f}% val)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============== SUBJECTS FELFEDEZ√âSE ===============\n",
    "\n",
    "print(\"\\nüîç Training subjects felfedez√©se...\")\n",
    "try:\n",
    "    all_subjects = discover_subjects(data_root, split='train')\n",
    "    print(f\"‚úì {len(all_subjects)} subject tal√°lva!\")\n",
    "    \n",
    "    # Oszt√°ly eloszl√°s\n",
    "    class_counts = {}\n",
    "    for subject in all_subjects.values():\n",
    "        class_name = subject['class_name']\n",
    "        class_counts[class_name] = class_counts.get(class_name, 0) + 1\n",
    "    \n",
    "    print(\"\\nüìä Oszt√°ly eloszl√°s:\")\n",
    "    for class_name, count in sorted(class_counts.items()):\n",
    "        print(f\"  {class_name}: {count}\")\n",
    "    \n",
    "    # Modalit√°s statisztik√°k\n",
    "    modality_counts = {'amp': 0, 'phase': 0, 'mask': 0}\n",
    "    for subject in all_subjects.values():\n",
    "        for modality in subject['modalities'].keys():\n",
    "            modality_counts[modality] += 1\n",
    "    \n",
    "    print(\"\\nüé≠ Modalit√°s lefedetts√©g:\")\n",
    "    for modality, count in modality_counts.items():\n",
    "        percentage = (count / len(all_subjects)) * 100\n",
    "        print(f\"  {modality}: {count}/{len(all_subjects)} ({percentage:.1f}%)\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Hiba az adatok bet√∂lt√©sekor: {e}\")\n",
    "    print(\"\\nEllen≈ërizd az adatok strukt√∫r√°j√°t!\")\n",
    "    print(\"V√°rt strukt√∫ra:\")\n",
    "    print(\"  data_root/\")\n",
    "    print(\"    train/\")\n",
    "    print(\"      class_chlorella/\")\n",
    "    print(\"        123_amp.png\")\n",
    "    print(\"        123_phase.png\")\n",
    "    print(\"        123_mask.png\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============== ADATOK BET√ñLT√âSE ===============\n",
    "\n",
    "data_root = CONFIG['data']['data_root']\n",
    "print(f\"üìÇ Adatok keres√©se: {data_root}\")\n",
    "\n",
    "# El√©rhet≈ë k√∂nyvt√°rak ellen≈ërz√©se\n",
    "if os.path.exists(data_root):\n",
    "    print(\"\\nüìÅ El√©rhet≈ë k√∂nyvt√°rak:\")\n",
    "    for item in sorted(os.listdir(data_root)):\n",
    "        item_path = os.path.join(data_root, item)\n",
    "        if os.path.isdir(item_path):\n",
    "            print(f\"  ‚îú‚îÄ {item}\")\n",
    "else:\n",
    "    print(f\"\\n‚ùå FIGYELEM: A '{data_root}' k√∂nyvt√°r nem tal√°lhat√≥!\")\n",
    "    print(\"\\nüí° √Åll√≠tsd be a helyes el√©r√©si utat:\")\n",
    "    print(\"  - Kaggle: /kaggle/input/your-dataset-name/\")\n",
    "    print(\"  - Colab: /content/your-dataset-folder/\")\n",
    "    print(\"\\nM√≥dos√≠tsd a 6. cell√°ban (Konfigur√°ci√≥) a CONFIG['data']['data_root'] √©rt√©k√©t!\")\n",
    "    raise FileNotFoundError(f\"Az adatok k√∂nyvt√°ra nem tal√°lhat√≥: {data_root}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============== TRAINING √ñSSZES FOLD-RA ===============\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"\\\\nEszk√∂z: {device}\")\n",
    "\n",
    "output_dir = ensure_dir(CONFIG['data']['output_dir'])\n",
    "checkpoints_dir = ensure_dir(output_dir / 'checkpoints')\n",
    "\n",
    "# Training minden fold-ra\n",
    "fold_results = []\n",
    "\n",
    "for fold_id, (train_sids, val_sids) in enumerate(folds):\n",
    "    print(f\"\\\\n{'='*60}\")\n",
    "    print(f\"FOLD {fold_id} TRAINING\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Subjects sz≈±r√©se\n",
    "    train_subjects = {sid: all_subjects[sid] for sid in train_sids}\n",
    "    val_subjects = {sid: all_subjects[sid] for sid in val_sids}\n",
    "    \n",
    "    # Datasets l√©trehoz√°sa\n",
    "    train_dataset = SubjectDataset(\n",
    "        train_subjects,\n",
    "        transform=get_train_transforms(CONFIG['data']['img_size']),\n",
    "        img_size=CONFIG['data']['img_size']\n",
    "    )\n",
    "    \n",
    "    val_dataset = SubjectDataset(\n",
    "        val_subjects,\n",
    "        transform=get_val_transforms(CONFIG['data']['img_size']),\n",
    "        img_size=CONFIG['data']['img_size']\n",
    "    )\n",
    "    \n",
    "    # DataLoaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=CONFIG['training']['batch_size'],\n",
    "        shuffle=True,\n",
    "        num_workers=CONFIG['data']['num_workers'],\n",
    "        pin_memory=True if torch.cuda.is_available() else False\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=CONFIG['training']['batch_size'],\n",
    "        shuffle=False,\n",
    "        num_workers=CONFIG['data']['num_workers'],\n",
    "        pin_memory=True if torch.cuda.is_available() else False\n",
    "    )\n",
    "    \n",
    "    # Model l√©trehoz√°sa\n",
    "    model = ChlorellaClassifier(\n",
    "        architecture=CONFIG['model']['architecture'],\n",
    "        num_classes=CONFIG['model']['num_classes'],\n",
    "        input_channels=CONFIG['model']['input_channels'],\n",
    "        pretrained=CONFIG['model']['pretrained']\n",
    "    ).to(device)\n",
    "    \n",
    "    # Training\n",
    "    fold_result = train_one_fold(\n",
    "        model, \n",
    "        train_loader, \n",
    "        val_loader, \n",
    "        fold_id, \n",
    "        CONFIG, \n",
    "        device, \n",
    "        checkpoints_dir\n",
    "    )\n",
    "    \n",
    "    fold_results.append(fold_result)\n",
    "    \n",
    "    # Mem√≥ria felszabad√≠t√°s\n",
    "    del model, train_loader, val_loader, train_dataset, val_dataset\n",
    "    torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "\n",
    "print(f\"\\\\n{'='*60}\")\n",
    "print(\"√ñSSZES FOLD TRAINING BEFEJEZVE\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéâ K√©sz!\n",
    "\n",
    "A pipeline sikeresen lefutott. Az eredm√©nyek:\n",
    "\n",
    "### üìÅ Mentett f√°jlok:\n",
    "- **Checkpointok**: `/kaggle/working/outputs/checkpoints/fold_X_best.pth`\n",
    "- **Training summary**: `/kaggle/working/outputs/reports/training_summary.json`\n",
    "- **Submission**: `/kaggle/working/outputs/submissions/submission.csv`\n",
    "\n",
    "### üìä K√∂vetkez≈ë l√©p√©sek:\n",
    "1. T√∂ltsd le a `submission.csv` f√°jlt\n",
    "2. Ellen≈ërizd a training metrik√°kat\n",
    "3. K√≠s√©rletezz a hyperparam√©terekkel (epochs, learning rate, augment√°ci√≥k)\n",
    "4. Pr√≥b√°lj m√°s architekt√∫r√°t (resnext50_32x4d, vgg11_bn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============== TEST SET INFERENCE ===============\n",
    "\n",
    "print(\"\\nüîÆ Test set inference elkezd√©se...\")\n",
    "\n",
    "# Test subjects bet√∂lt√©se\n",
    "try:\n",
    "    test_root = Path(CONFIG['data']['data_root']) / 'test'\n",
    "    print(f\"üîç Test mapp√°t keresem: {test_root}\")\n",
    "    \n",
    "    if not test_root.exists():\n",
    "        raise FileNotFoundError(f\"Test mappa nem tal√°lhat√≥: {test_root}\")\n",
    "    \n",
    "    # Ellen≈ërizz√ºk hogy vannak-e PNG f√°jlok\n",
    "    png_count = len(list(test_root.glob('*.png')))\n",
    "    if png_count == 0:\n",
    "        png_count = len(list(test_root.glob('**/*.png')))\n",
    "    \n",
    "    print(f\"üìä {png_count} PNG f√°jl tal√°lhat√≥ a test mapp√°ban\")\n",
    "    \n",
    "    test_subjects = discover_subjects(CONFIG['data']['data_root'], split='test')\n",
    "    print(f\"‚úì {len(test_subjects)} test subject tal√°lva!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Test set nem tal√°lhat√≥: {e}\")\n",
    "    print(\"Kihagyom a test inference-t.\")\n",
    "    test_subjects = {}\n",
    "\n",
    "if test_subjects:\n",
    "    # Test dataset l√©trehoz√°sa\n",
    "    test_dataset = SubjectDataset(\n",
    "        test_subjects,\n",
    "        transform=get_val_transforms(CONFIG['data']['img_size']),\n",
    "        img_size=CONFIG['data']['img_size']\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=CONFIG['training']['batch_size'],\n",
    "        shuffle=False,\n",
    "        num_workers=CONFIG['data']['num_workers'],\n",
    "        pin_memory=True if torch.cuda.is_available() else False\n",
    "    )\n",
    "    \n",
    "    # Ensemble prediction - haszn√°ljuk az √∂sszes fold legjobb modellj√©t\n",
    "    all_predictions = []\n",
    "    all_subject_ids = list(test_subjects.keys())\n",
    "    \n",
    "    print(f\"\\nü§ñ Ensemble prediction {len(fold_results)} modellel...\")\n",
    "    \n",
    "    for fold_id in range(len(fold_results)):\n",
    "        checkpoint_path = checkpoints_dir / f'fold_{fold_id}_best.pth'\n",
    "        \n",
    "        if not checkpoint_path.exists():\n",
    "            print(f\"‚ö†Ô∏è Fold {fold_id} checkpoint nem tal√°lhat√≥: {checkpoint_path}\")\n",
    "            continue\n",
    "        \n",
    "        # Model bet√∂lt√©se\n",
    "        model = ChlorellaClassifier(\n",
    "            architecture=CONFIG['model']['architecture'],\n",
    "            num_classes=CONFIG['model']['num_classes'],\n",
    "            input_channels=CONFIG['model']['input_channels'],\n",
    "            pretrained=False  # Nem kell pretrained, mert bet√∂ltj√ºk a s√∫lyokat\n",
    "        ).to(device)\n",
    "        \n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        model.eval()\n",
    "        \n",
    "        # Predikci√≥\n",
    "        fold_probs = []\n",
    "        with torch.no_grad():\n",
    "            for inputs, _ in tqdm(test_loader, desc=f'Fold {fold_id} inference'):\n",
    "                inputs = inputs.to(device)\n",
    "                outputs = model(inputs)\n",
    "                probs = torch.softmax(outputs, dim=1)\n",
    "                fold_probs.extend(probs.cpu().numpy())\n",
    "        \n",
    "        all_predictions.append(np.array(fold_probs))\n",
    "        \n",
    "        del model\n",
    "        torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "    \n",
    "    # Ensemble: √°tlagoljuk a val√≥sz√≠n≈±s√©geket\n",
    "    ensemble_probs = np.mean(all_predictions, axis=0)\n",
    "    ensemble_preds = np.argmax(ensemble_probs, axis=1)\n",
    "    \n",
    "    # Submission DataFrame l√©trehoz√°sa\n",
    "    submission_df = pd.DataFrame({\n",
    "        'subject_id': all_subject_ids,\n",
    "        'predicted_class': ensemble_preds\n",
    "    })\n",
    "    \n",
    "    # Submission ment√©se\n",
    "    submissions_dir = ensure_dir(output_dir / 'submissions')\n",
    "    submission_path = submissions_dir / 'submission.csv'\n",
    "    submission_df.to_csv(submission_path, index=False)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Submission elk√©sz√ºlt!\")\n",
    "    print(f\"üíæ Mentve: {submission_path}\")\n",
    "    print(f\"üìä {len(submission_df)} predikci√≥\")\n",
    "    \n",
    "    # Predikci√≥ eloszl√°s\n",
    "    pred_counts = submission_df['predicted_class'].value_counts().sort_index()\n",
    "    print(\"\\nüìà Predikci√≥ eloszl√°s:\")\n",
    "    for class_id, count in pred_counts.items():\n",
    "        class_name = CLASS_ID_TO_NAME.get(class_id, 'unknown')\n",
    "        percentage = (count / len(submission_df)) * 100\n",
    "        print(f\"   ‚Ä¢ {class_name}: {count} ({percentage:.1f}%)\")\n",
    "    \n",
    "    # El≈ën√©zet\n",
    "    print(\"\\nüëÄ Submission el≈ën√©zet (els≈ë 10 sor):\")\n",
    "    print(submission_df.head(10).to_string(index=False))\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Test set nem el√©rhet≈ë - submission f√°jl nem k√©sz√ºlt.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Test Set Inference √©s Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============== EREDM√âNYEK √ñSSZEGZ√âSE ===============\n",
    "\n",
    "import json\n",
    "\n",
    "# Metrik√°k aggreg√°l√°sa\n",
    "f0_5_scores = [result['best_f0_5'] for result in fold_results]\n",
    "best_epochs = [result['best_epoch'] for result in fold_results]\n",
    "\n",
    "summary = {\n",
    "    'num_folds': len(fold_results),\n",
    "    'architecture': CONFIG['model']['architecture'],\n",
    "    'avg_f0_5': float(np.mean(f0_5_scores)),\n",
    "    'std_f0_5': float(np.std(f0_5_scores)),\n",
    "    'min_f0_5': float(np.min(f0_5_scores)),\n",
    "    'max_f0_5': float(np.max(f0_5_scores)),\n",
    "    'fold_scores': [float(score) for score in f0_5_scores],\n",
    "    'fold_best_epochs': [int(epoch) for epoch in best_epochs]\n",
    "}\n",
    "\n",
    "# Ment√©s JSON-ba\n",
    "reports_dir = ensure_dir(output_dir / 'reports')\n",
    "summary_path = reports_dir / 'training_summary.json'\n",
    "\n",
    "with open(summary_path, 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "# Eredm√©nyek ki√≠r√°sa\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìä TRAINING EREDM√âNYEK √ñSSZEFOGLAL√ÅSA\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nüèóÔ∏è  Architekt√∫ra: {summary['architecture']}\")\n",
    "print(f\"üìÅ Fold-ok sz√°ma: {summary['num_folds']}\")\n",
    "print(f\"\\nüéØ Chlorella F0.5 Score:\")\n",
    "print(f\"   ‚Ä¢ √Åtlag: {summary['avg_f0_5']:.4f} ¬± {summary['std_f0_5']:.4f}\")\n",
    "print(f\"   ‚Ä¢ Min:   {summary['min_f0_5']:.4f}\")\n",
    "print(f\"   ‚Ä¢ Max:   {summary['max_f0_5']:.4f}\")\n",
    "print(f\"\\nüìà Fold-onk√©nti eredm√©nyek:\")\n",
    "for fold_id, (score, epoch) in enumerate(zip(f0_5_scores, best_epochs)):\n",
    "    print(f\"   ‚Ä¢ Fold {fold_id}: F0.5 = {score:.4f} (epoch {epoch})\")\n",
    "\n",
    "print(f\"\\nüíæ Eredm√©nyek mentve: {summary_path}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Eredm√©nyek √ñsszegz√©se"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
